{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy \n",
    "\n",
    "On their website, Spacy has an interactive course structure for beginners to perform nlp tasks using spacy. This notebook tries to understand and perform those tasks and prepare a comprehensive summary of those chapters.\n",
    "\n",
    "You can view it here: https://course.spacy.io/chapter1\n",
    "\n",
    "This notebook contains a brief synopsis of all the concepts covered in the first three chapters of this interactive course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nlp() object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import english language class\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object.\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nlp object contains the processing pipeline. It contains language specific rules for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "your\n",
      "father\n",
      "!\n",
      "This\n",
      "is\n",
      "the\n",
      "year\n",
      "2019\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I am your father! This is the year 2019...\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to word_tokenize in nltk library. It tokenizes the text by default when you pass it to the nlp object which in our case is English(). You can now get a slice of the text from above nlp() object or even a single word from the nlp object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your\n",
      "father\n"
     ]
    }
   ],
   "source": [
    "span = doc[2:4]\n",
    "for i in span:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Attributes \n",
    "\n",
    "You can check whether a word is a punctuation or an alphabet or a number simply using a predefined function.\n",
    "\n",
    "- is_punct: To check if a token is a punctuation \n",
    "- is_alpha: To check if a token is an English word \n",
    "- like_num: To check if a token is a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : ['I', 'am', 'your', 'father', '!', 'This', 'is', 'the', 'year', '2019', '...']\n",
      "\n",
      "Numeric: ['2019']\n",
      "\n",
      "Punctuations: ['!', '...']\n",
      "\n",
      "Words: ['I', 'am', 'your', 'father', 'This', 'is', 'the', 'year']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All tokens\n",
    "print(\"Tokens : {}\\n\".format([i.text for i in doc]))\n",
    "\n",
    "# Get numbers only\n",
    "print(\"Numeric: {}\\n\".format([i.text for i in doc if i.like_num]))\n",
    "\n",
    "# Get punctuations only\n",
    "print(\"Punctuations: {}\\n\".format([i.text for i in doc if i.is_punct]))\n",
    "\n",
    "# Get alphabets/words only\n",
    "print(\"Words: {}\\n\".format([i.text for i in doc if i.is_alpha]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tokens index while iterating over an nlp object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 0],\n",
       " ['am', 1],\n",
       " ['your', 2],\n",
       " ['father', 3],\n",
       " ['!', 4],\n",
       " ['This', 5],\n",
       " ['is', 6],\n",
       " ['the', 7],\n",
       " ['year', 8],\n",
       " ['2019', 9],\n",
       " ['...', 10]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[token.text, token.i] for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Statistical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has the following three features on which it has already run multiple training instances and works well. But one can also update/train it in a custom way to identify their own peculiar field.\n",
    "\n",
    "- Part of Speech Tags\n",
    "- Syntactic Dependencies\n",
    "- Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# en_core_web_sm is a small(sm) English model that supports all core capabilities and is trained a lot on web text.\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tag\n",
    "\n",
    "Link - https://spacy.io/usage/linguistic-features#pos-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"spacy_parts_of_speech.PNG\" w=500 h=100> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"There's a feeling within me, an everglow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There      ADV\n",
      "'s         VERB\n",
      "a          DET\n",
      "feeling    NOUN\n",
      "within     ADP\n",
      "me         PRON\n",
      ",          PUNCT\n",
      "an         DET\n",
      "everglow   NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(\"{0:<10}\".format(token.text), token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Dependencies Prediction\n",
    "In addition to predicting the part of speech, spacy also returns the predicted dependency of a word i.e. whether it's a subject or an object or a verb that connects the two etc. Here's how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There           ADV      expl    's\n",
      "'s              VERB     ROOT    's\n",
      "a               DET      det     feeling\n",
      "feeling         NOUN     attr    's\n",
      "within          ADP      prep    feeling\n",
      "me              PRON     pobj    within\n",
      ",               PUNCT    punct   feeling\n",
      "an              DET      det     everglow\n",
      "everglow        NOUN     conj    feeling\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print('{0:<15}'.format(token.text), '{0:8}'.format(token.pos_), '{0:7}'.format(token.dep_), token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Named entities are groups/categories which a certain class of words belong to. {Brady, Destin, Derek} these will form a part of a named entity called <button>name</button> or person, {Australia, Alabama, Kingston} these will be a part of another named entity called <button>places</button> and so on.\n",
    "\n",
    "Link - Link - https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama  GPE\n",
      "Australia GPE\n",
      "the Coriolis Effect FAC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Go to Alabama and Australia and use a sink to test the Coriolis Effect.\")\n",
    "for token in doc.ents:\n",
    "    print('{0:<8}'.format(token.text), token.label_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside Tip**\n",
    "\n",
    "Use the explain method to get the meaning of common tags and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Buildings, airports, highways, bridges, etc.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('FAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based Matching\n",
    "\n",
    "You can match patterns based on parts of speech, lemmas, text, named_entities etc. i.e. You can make your own pattern and get text strings or values out of any given document based on this pattern.\n",
    "\n",
    "Import matcher object and use it to accomplish the above task.\n",
    "\n",
    "You can give multiple match patterns to the matcher object.\n",
    "\n",
    "You can call this on your document and then use the returned object by the match function to extract the tags which have matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'LEMMA':'love', 'POS':'VERB'}, {'POS':'PROPN'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern above says that match the substring which has any form of the word \"love\" followed by a noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I loved Kevin Spacey as Keyser Soze in The Usual Suspects 1995 but post 2017 that love has dwindled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved Kevin\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "mt = Matcher(nlp.vocab)\n",
    "mt.add('Pattern 1', None, pattern)\n",
    "matches = mt(doc)    \n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          PRON\n",
      "loved      VERB\n",
      "Kevin      PROPN\n",
      "Spacey     PROPN\n",
      "as         ADP\n",
      "Keyser     PROPN\n",
      "Soze       PROPN\n",
      "in         ADP\n",
      "The        DET\n",
      "Usual      ADJ\n",
      "Suspects   NOUN\n",
      "1995       NUM\n",
      "but        CCONJ\n",
      "post       VERB\n",
      "2017       NUM\n",
      "that       ADP\n",
      "love       NOUN\n",
      "has        VERB\n",
      "dwindled   VERB\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print('{0:<10}'.format(token.text), token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern2 = [{'LOWER':'the'}, {'LOWER':'usual'}, {'LOWER':'suspects'}, {'IS_DIGIT':True}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved Kevin\n",
      "The Usual Suspects 1995\n"
     ]
    }
   ],
   "source": [
    "mt.add('MovieTitle', None, pattern2)\n",
    "matches = mt(doc)\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Rule_Baded_Matching.PNG\" w=500 h=50> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Datastructures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vocab: stores data shared across multiple documents\n",
    "- To save memory, spaCy encodes all strings to hash values\n",
    "- Strings are only stored once in the StringStore via nlp.vocab.strings\n",
    "- String store: lookup table in both directions\n",
    "\n",
    "spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string store is available as nlp dot vocab dot strings.\n",
    "\n",
    "It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. \n",
    "\n",
    "**NOTE: YOU NEED TO ADD THE WORD TO AN NLP OBJECT BEFORE GENERATING A HASH FOR THAT WORD ELSE, IT WILL THROW AN ERROR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Badreesh 5385398226147478911\n",
      "Vinayak 16178419844459703194\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Badreesh Vinayak')\n",
    "for tokens in doc:\n",
    "    student_hash = nlp.vocab.strings[tokens.text]\n",
    "    student_string = nlp.vocab.strings[student_hash]\n",
    "    print(student_string, student_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Doc Object\n",
    "\n",
    "Here we're creating a Doc from three words. The spaces are a list of boolean values indicating whether the word is followed by a space. Every token includes that information â€“ even the last one!\n",
    "\n",
    "The Doc class takes three arguments: the shared vocab, the words and the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "# Spaces - Whether there's a space after the respective word or not.\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Span Object\n",
    "\n",
    "To create a Span manually, we can also import the class from spacy dot tokens. We can then instantiate it with the doc and the span's start and end index.\n",
    "\n",
    "To add an entity label to the span, we first need to look up the string in the string store. We can then provide it to the span as the label argument.\n",
    "\n",
    "The doc dot ents are writable, so we can add entities manually by overwriting it with a list of spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lexeme Object\n",
    "\n",
    "The object returned after looking up for an item in the vocabulary is called a lexeme. They hold the text, the hash and attributes like is_alpha, like_num etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:           Tom\n",
      "Hash:           6005358355014000477\n",
      "Is aphabet:     True\n",
      "Is Numeric:     False\n",
      "Is Punctuation: False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tom Hanks is a great guy. I like him.\")\n",
    "lexeme = nlp.vocab['Tom']\n",
    "print(\"{0:<15}\".format(\"Text:\"), lexeme.text)\n",
    "print(\"{0:<15}\".format(\"Hash:\"), lexeme.orth)\n",
    "print(\"{0:<15}\".format(\"Is aphabet:\"), lexeme.is_alpha)\n",
    "print(\"{0:<15}\".format(\"Is Numeric:\"), lexeme.like_num)\n",
    "print(\"{0:<15}\".format(\"Is Punctuation:\"), lexeme.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors and Semantic Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the similarity between sentences based on spacy's similarity based on word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the medium version of spacy vocab\n",
    "# Do this to load word vectors which come with the medium version and not with small version\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing two sentences/two documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8885209527571564\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Naruto is a great anime.\")\n",
    "doc2 = nlp(\"Dragon Ball Z is an awesome anime.\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing a token in a sentence/document with another token from another sentence/document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49898103\n"
     ]
    }
   ],
   "source": [
    "print(doc1[0].similarity(doc2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing a span with a document** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7416453577383586\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Naruto and Pokemon are awesomoe anime\")\n",
    "doc2 = nlp(\"Dragon Ball Z and Duel Masters are great Anime\")\n",
    "\n",
    "print(doc1[3:].similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wordVectors_Spacy.PNG\" w=100 h=100> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19859 , -0.062818, -0.36614 , -0.41786 ,  0.20962 , -0.26728 ,\n",
       "        0.246   ,  0.12783 , -0.045845,  2.5253  ], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1[3].vector[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caveat about word similarity**\n",
    "\n",
    "Similarity is highly context-specific. Look at the following example. Although one has a positive sentiment and another has a negative sentiment associated with them, they both are talking about sentiment held by a person toward a computer in which case the nature of two statements are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9524976951533566"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = nlp(\"I love computers.\")\n",
    "d2 = nlp(\"I hate computers.\")\n",
    "d1.similarity(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Models and Rule Based Matching\n",
    "\n",
    "Combining these two is greatly helpful in many ways. The following table summarizes how so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stats_vs_rules.png\" w=200 h=100>\n",
    "\n",
    "After doing rule based matching, you can find out different attributes of the individual tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Span: successful chap\n",
      "\n",
      "Match ID: 6509405884090561913\n",
      "\n",
      "Start Token: 5\n",
      "\n",
      "End Token: 7\n",
      "\n",
      "Root token:  chap\n",
      "\n",
      "Root head token:  is\n",
      "\n",
      "Previous token:  extremely\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "mt = Matcher(nlp.vocab)\n",
    "mt.add('Pat1', None, [{'LOWER':'successful'}, {'LOWER':'chap'}])\n",
    "doc = nlp(\"Simon Cowell is an extremely successful chap\")\n",
    "\n",
    "for mt_id, start, end in mt(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched Span:', span.text)\n",
    "    print('\\nMatch ID:', mt_id)\n",
    "    print('\\nStart Token:', start)\n",
    "    print('\\nEnd Token:', end)\n",
    "    \n",
    "    # Get the span's root and root head tokens\n",
    "    print('\\nRoot token: ', span.root.text)\n",
    "    print('\\nRoot head token: ', span.root.head.text)\n",
    "    \n",
    "    # Previous Token\n",
    "    print(\"\\nPrevious token: \", doc[start-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Matcher\n",
    "It is similar to Rule based matching but instead of the patterns, we are going to pass a doc object to the Matcher. It is fast and efficient and easier to understand than rule-based matching technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Span: Simon Cowell\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "mt = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Simon Cowell\")\n",
    "mt.add(\"Celebrity\", None, pattern)\n",
    "doc = nlp(\"Simon Cowell is a great chap.\")\n",
    "\n",
    "for mt_id, start, end in mt(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched Span:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "In the doc provided below:\n",
    "\n",
    "Create a <button>Pattern1</button> so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
    "\n",
    "Create a <button>Pattern2</button> so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"},{'IS_PUNCT':True},{'LOWER':'free'}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Pipelines\n",
    "\n",
    "*What does the nlp object actually do?*\n",
    "\n",
    "First, the tokenizer is applied to turn the string of text into a Doc object. Next, a series of pipeline components is applied to the Doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed Doc is returned, so you can work with it.\n",
    "\n",
    "The following are different components of the processing pipeline.\n",
    "<img src = \"pipeline_1.png\" w = 100 h = 100>\n",
    "\n",
    "Descriptions of the above components is shown below\n",
    "\n",
    "- The *part-of-speech tagger* sets the *token dot tag attribute.\n",
    "\n",
    "\n",
    "- The *dependency parser* adds the *token dot dep* and *token dot head* attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "\n",
    "\n",
    "- The *named entity recognizer* adds the detected entities to the *doc dot ents* property. It also sets *entity type attributes* on the tokens that indicate if a token is part of an entity or not.\n",
    "\n",
    "\n",
    "- Finally, the *text classifier* sets *category labels* that apply to the whole text, and adds them to the *doc dot cats* property.\n",
    "\n",
    "\n",
    "- Because text categories are always very specific, the text classifier is not included in any of the pre-trained models by default. But you can use it to train your own system.\n",
    "\n",
    "\n",
    "<img src = \"pipeline_components.png\" w = 100 h = 100>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x2181d9c32e8>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x2181d8c2108>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x2181d8c2168>)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pipeline Components\n",
    "\n",
    "You can create a custom component or a custom function and add it to the pipeline at any stage in the pipeline that you want to. There are ways of adding a custom component which are as follows:\n",
    "<img src=\"pipeline_custom_component.png\" w=200 h=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_component(doc):\n",
    "    length = len(doc)\n",
    "    print(\"Doc: {}\".format(doc), \"\\nThe length of this document is: {}\".format(length))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(new_component, first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: Pokemon is the best anime ever. \n",
      "The length of this document is: 7\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Pokemon is the best anime ever.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "They can be used in order to add meta_data to docs, tokens and spans. The data may be added at the start. It can also be overwritten/changed subsequently at a later stage.\n",
    "\n",
    "They can be accessed using the ._ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extensions on the Doc, Token and Span\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: Mr Blue Sky Please tell us why \n",
      "The length of this document is: 7\n"
     ]
    }
   ],
   "source": [
    "# Overwrite an extension\n",
    "doc = nlp(\"Mr Blue Sky Please tell us why\")\n",
    "doc[1]._.is_color = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token._.is_color:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define a custom getter and setter method for the meta-attribute value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : Blue\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define a getter\n",
    "def get_col(doc):\n",
    "    colors = ['orange', 'blue', 'purple']\n",
    "    return doc.text.lower() in colors\n",
    "\n",
    "Token.set_extension('is_color', getter = get_col, force = True)\n",
    "\n",
    "print(doc[1]._.is_color, ':', doc[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define a custom method for the global token class which every token in the scope will have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: Mr. Blue sky please tell us why... \n",
      "The length of this document is: 8\n",
      "True - blue\n",
      "True sky\n"
     ]
    }
   ],
   "source": [
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text.lower() for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "Doc.set_extension('has_token_', method = has_token)\n",
    "\n",
    "doc = nlp(\"Mr. Blue sky please tell us why...\")\n",
    "\n",
    "print(doc._.has_token_('blue'), '- blue')\n",
    "\n",
    "print(doc._.has_token_('sky'), 'sky')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code shows the power of extensions. It creates a search string of Wikipedia for a person in the document provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years     DATE                 None\n",
      "first                ORDINAL              None\n",
      "David Bowie          PERSON               https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(\"{0:<20}\".format(ent.text), \"{0:<20}\".format(ent.label_), ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Performance.\n",
    "\n",
    "Techniques to make spacy run as fast as possible for processing large amount of text.\n",
    "\n",
    "### nlp.pipe()\n",
    "\n",
    "If you need to process a lot of texts and create a lot of Doc objects in a row, the nlp dot pipe method can speed this up significantly.\n",
    "\n",
    "**It processes the texts as a stream and yields Doc objects.**\n",
    "\n",
    "**It is much faster than just calling nlp on each text, because it batches up the texts.**\n",
    "\n",
    "nlp dot pipe is a generator that yields Doc objects, so in order to get a list of Docs, remember to call the list method around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[You are awesome,\n",
       " I love the way you handle things,\n",
       " I am honored to be with you]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['You are awesome', 'I love the way you handle things', 'I am honored to be with you']\n",
    "# BAD - [nlp(doc) for doc in docs]\n",
    "docs = list(nlp.pipe(docs))\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp dot pipe also supports passing in tuples of text / context if you set \"as tuples\" to True.\n",
    "\n",
    "The method will then yield doc / context tuples.\n",
    "\n",
    "This is useful for passing in additional metadata, like an ID associated with the text, or a page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Meta data\n",
    "You can even add the context meta data to custom attributes.\n",
    "\n",
    "In this example, we're registering two extensions, \"id\" and \"page number\", which default to None.\n",
    "\n",
    "After processing the text and passing through the context, we can overwrite the doc extensions with our context metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension('id', default=None)\n",
    "Doc.set_extension('page_number', default=None)\n",
    "\n",
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context['id']\n",
    "    doc._.page_number = context['page_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Pipe Disabling\n",
    "\n",
    "After the with block, the disabled pipeline components are automatically restored.\n",
    "\n",
    "In the with block, spaCy will only run the remaining components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are amazing. ()\n"
     ]
    }
   ],
   "source": [
    "# Disable tagger and parser\n",
    "text = \"You are amazing.\"\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.text, doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "297.275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
